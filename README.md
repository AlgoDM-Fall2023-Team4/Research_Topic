# Research_Topic
----- 
> <br>

> [Codelab Slides]() <br>
> [Application Demo/Presentation]() <br>

## Index
  - [Objective](#objective)
  - [Project Structure](#project-structure)
  - [How to run the applications](#how-to-run-the-application-locally)
----- 

## Objective
  This project is made for getting answering for a query or prompt given by the user where the models are trained on the external datasets and have their own memory. Here we have used multiple LLM models like
  - Langchain
  - Classical Model
  - Retrieval Augumented Generation<br>

  ## Project Structure
```
  LLM Models/
│
├── src/             
│   ├── attribute_extraction.py/
│   ├── streamlit              # Directory to streamlit app
│       ├── main.py
├── RAG_model.py               # RAG model python file
├── README.txt                 # Project README file
│
├── Smart_Watch_Review.csv/    # input data
├── combined_reviews_with_ratings.txt/      # file generated by running RAG model to feed the models
├── output.json                              # file created to structure the excel sheet into dict format 
├── requirements.txt    # Project-wide requirements file

## How to run the application
- Clone the repo to get all the source code on your machine

```bash
git clone https://github.com/AlgoDM-Fall2023-Team4/Research_Topic.git
```
- All the code related to the streamlit is in the streamlit/ directory of the project

- First, create a virtual environment, activate and install all requirements from the requirements.txt file present
```bash
python -m venv <virtual_environment_name>
```
```bash
source <virtual_environment_name>/bin/activate
```
```bash
pip install -r <path_to_requirements.txt>
```
- Run the application

```bash
streamlit run streamlit/main.py
```

- To run the RAG model use colab notebook
